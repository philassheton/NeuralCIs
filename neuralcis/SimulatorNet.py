from neuralcis.DataSaver import DataSaver
from neuralcis.FiftyFiftyLayer import FiftyFiftyLayer

import tensorflow as tf
import tensorflow_probability as tfp                                           # type: ignore
import numpy as np

from typing import Optional, Tuple, Callable
from neuralcis.common import Samples, NetInputs, NetOutputs, NodesInLayer
from neuralcis.common import NetOutputBlob
from neuralcis.common import TrainingBatches
from tensor_annotations.tensorflow import Tensor1, Tensor2
import tensor_annotations.tensorflow as ttf
tf32 = ttf.float32


@tf.function
def run_net(net, x, training=tf.constant(False)):
    return net(x, training=training)


class SimulatorNet(DataSaver):
    """Train a multilayer perceptron based on data generated by simulation.

    This is where the actual neural networks live, and at some point will also
    be added features for automatically searching for best hyperparameters,
    etc.  You just have to construct the object with the following inputs, and
    then call the fit method:

    :param sampling_distribution_fn: A function that takes as input a single
    int and generates a tensor of input data and (optionally) a tensor of
    target outputs (if those are required by the loss function).  If no target
    outputs are provided, they should be replaced with a None value.
    :param validation_set_fn: A function that takes no arguments and returns a
    static validation set in the same format as from sampling_distribution_fn.
    This needs to be a function as the simulation itself may depend on other
    external factors.
    :param loss_fn: A function that takes the output from the run_net_fn and
    the target values from the sampling_distribution_fn and calculates a loss
    value.
    :param run_net_fn: OPTIONAL - A function that takes a tf.keras.Sequential
    object and a set of inputs (plus an optional "training" arg) and applies
    the inputs to the network.  This only need be passed in if it is necessary
    to pull other numbers from the neural network than just its outputs, in
    order to calculate the loss.
    :param filename: If the network has been fit previously, and weights were
    saved, they can be loaded in the constructor by passing in the filename
    here.
    """
    def __init__(
            self,
            sampling_distribution_fn: Callable[
                [ttf.int32],                                       # n
                Tuple[
                    Tensor2[tf32, Samples, NetInputs],             # ins
                    Optional[Tensor2[tf32, Samples, NetOutputs]]   # targets
                ]],
            validation_set_fn: Callable[
                [],
                Tuple[
                    Tensor2[tf32, Samples, NetInputs],             # ins
                    Optional[Tensor2[tf32, Samples, NetOutputs]]   # targets
                ]],
            loss_fn: Callable[
                [NetOutputBlob,                                    # outputs
                 Optional[Tensor2[tf32, Samples, NetOutputs]]],    # targets
                ttf.float32                                        # loss
            ],
            run_net_fn: Callable[
                [tf.keras.Sequential,                              # net
                 Tensor2[tf32, Samples, NetInputs],                # ins
                 ttf.bool],                                        # training
                NetOutputBlob
            ] = run_net,
            filename: str = ""
    ) -> None:

        self.sampling_distribution_fn = sampling_distribution_fn
        self.validation_set_fn = validation_set_fn
        self.loss_fn = loss_fn
        self.run_net_during_training_fn = run_net_fn

        self.net = self.create_net()

        # not filled in at init because it is slow / not always needed, so it
        #   needs to be explicitly filled by calling initialise_for_training
        self.validation_optimum_loss = tf.Variable(np.nan)

        self.optimizer = tf.keras.optimizers.Nadam()

        # TODO: Wrap these lines of code up in a func so they are not repeated.
        validation_ins, validation_targets = self.validation_set_fn()
        initial_validation_outs = self.run_net_during_training_fn(
            self.net,
            validation_ins,
            tf.constant(False)
        )
        initial_loss = self.loss_fn(initial_validation_outs,
                                    validation_targets)

        self.validation_loss_so_far = tf.Variable(initial_loss)
        self.batches_since_optimum = tf.Variable(0)

        super().__init__(
            filename,
            instance_tf_variables_to_save=[
                "validation_optimum_loss",
                "validation_loss_so_far"
            ],
            net_with_weights_to_save=self.net
        )

    def create_net(self) -> tf.keras.Sequential:
        net = tf.keras.models.Sequential([
            FiftyFiftyLayer(50),
            FiftyFiftyLayer(50),
            FiftyFiftyLayer(50),
            FiftyFiftyLayer(50),
            tf.keras.layers.Dense(
                1,
                kernel_initializer=tf.keras.initializers.GlorotUniform(),
                bias_initializer="zeros"
            )
        ])

        # Do this to force it to instantiate some weights
        net_ins, net_outs = self.sampling_distribution_fn(tf.constant(2))
        net(net_ins)

        return net

    def set_learning_rate(
            self,
            learning_rate: Optional[float] = None
    ) -> None:

        if learning_rate is None:
            learning_rate = self.optimizer.learning_rate
        self.optimizer.lr.assign(learning_rate)

    def fit(
            self,
            max_epochs: int,
            minibatch_size: int = 32,
            learning_rate_initial: float = 1e-3,
            divide_after_flattening_for: int = 10
    ) -> None:

        learning_rate = learning_rate_initial
        for epoch in range(max_epochs):
            self.set_learning_rate(learning_rate)
            learning_rate_check = self.optimizer.learning_rate.numpy()
            print(
                "Learning rate %f -- loss to beat is %f" %
                (learning_rate_check, self.validation_loss_so_far)             # type: ignore
            )
            validation_losses = self.fit_tf(
                num_minibatches_per_batch=tf.constant(100),
                num_batches=tf.constant(20),
                minibatch_size=tf.constant(minibatch_size)
            )

            if tfp.stats.variance(validation_losses) < 1.:
                break

            if self.batches_since_optimum > divide_after_flattening_for:       # type: ignore
                learning_rate = learning_rate / 2.

    # a "batch" is the amount it does before reporting learning rates etc
    @tf.function
    def fit_tf(
            self,
            num_minibatches_per_batch: ttf.int32,
            num_batches: ttf.int32,
            minibatch_size: ttf.int32
    ) -> Tensor1[tf32, TrainingBatches]:
        optimum_text = tf.cond(
            tf.math.is_nan(self.validation_optimum_loss),
            lambda: "",
            lambda: tf.strings.format(
                " (vs optimum: {})",
                self.validation_optimum_loss
            )
        )
        validation_losses = tf.TensorArray(tf.float32, num_batches,
                                           element_shape=())
        for batch in tf.range(num_batches):
            for minibatch in tf.range(num_minibatches_per_batch):
                net_ins, net_targets = self.sampling_distribution_fn(
                    minibatch_size
                )
                loss, grads = self.loss_and_gradient(
                    net_ins,
                    net_targets,
                    tf.constant(True)
                )
                self.optimizer.apply_gradients(zip(
                    grads,
                    self.net.trainable_weights
                ))

            validation_ins, validation_targets = self.validation_set_fn()
            validation_outs = self.run_net_during_training_fn(
                self.net,
                validation_ins,
                tf.constant(False)
            )
            validation_loss = self.loss_fn(validation_outs, validation_targets)
            validation_losses = validation_losses.write(batch, validation_loss)

            tf.print(tf.strings.format(
                "Training loss: {}{}",
                (validation_loss, optimum_text)
            ))

            batches_since_optimum = tf.cond(
                validation_loss < self.validation_loss_so_far,                 # type: ignore
                lambda: 0,
                lambda: self.batches_since_optimum + 1                         # type: ignore
            )
            validation_loss_so_far = tf.math.minimum(
                self.validation_loss_so_far,
                validation_loss
            )

            self.batches_since_optimum.assign(batches_since_optimum)
            self.validation_loss_so_far.assign(validation_loss_so_far)

        return validation_losses.stack()

    @tf.function
    def call_tf(
            self,
            net_ins: Tensor2[tf32, Samples, NetInputs]
    ) -> Tensor2[tf32, Samples, NetOutputs]:

        return self.net(net_ins)

    @tf.function
    def loss_and_gradient(
            self,
            net_ins: Tensor2[tf32, Samples, NetInputs],
            net_targets: Tensor2[tf32, Samples, NetOutputs],
            training: ttf.bool = tf.constant(False)
    ) -> Tuple[ttf.float32, list]:
        print("z loss_and_gradient")
        with tf.GradientTape() as tape2:                                       # type: ignore
            tape2.watch(self.net.trainable_weights)
            net_outs = self.run_net_during_training_fn(
                self.net,
                net_ins,
                training
            )
            loss = self.loss_fn(net_outs, net_targets)

        gradient = tape2.gradient(loss, self.net.trainable_weights)
        return loss, gradient

    @tf.function
    def get_layer_outputs_variance(
            self,
            layer_out: Tensor2[tf32, Samples, NodesInLayer]
    ) -> Tensor1[tf32, NodesInLayer]:
        return tfp.stats.variance(layer_out, sample_axis=0)

    def get_neuron_variances_for_validation_set(self) -> list:

        extractor = tf.keras.models.Model(
            inputs=self.net.input,
            outputs=[layer.output for layer in self.net.layers]
        )
        validation_ins, validation_outs = self.validation_set_fn()
        layer_outs = extractor(validation_ins)
        layer_variances = [
            tfp.stats.variance(out, sample_axis=0) for out in layer_outs
        ][0:-1]

        return layer_variances

    def set_validation_optimum_loss(
            self,
            validation_optimum_loss: Tensor1[tf32, Samples]
    ) -> None:

        self.validation_optimum_loss.assign(validation_optimum_loss)
