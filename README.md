# Simultaneous Confidence Intervals through Neural Networks

Generate simultaneous confidence intervals for almost any set of estimands using neural networks.

This is a work in progress;  the current version only handles a simple (and somewhat trivial) case.  Please return here in a few months' time, when I hope the project will be completed.  

## The Current Version

This project does not yet have a version number, as it the first version is still being finalised.  Version 1.0.0 is almost complete; the code works now, but just needs some finishing touches, and the interface might change in small ways until then.  

In Version 1.0.0, you will be able to generate p-values and confidence intervals for any single quantity whose sampling distribution depends only on one unknown parameter.  Known parameters, such as sample size, can also be included.  In the simple example below, confidence intervals are calculated for a Cohen's *d*, given the observed *d* value and the sample sizes of the two groups.  

In future versions (and **this is the real motivator of this project**), it will be possible to handle simultaneous confidence intervals on multiple quantities, also in the presence of nuisance variables (initial experiments in this direction are in the `experimental` branch of the project, but may not be stable or even functioning).

There is still considerable work to be done improving the fit of the networks, and in testing them, so p-values and CIs at present do not come with any guarantees of accuracy.

### How to use NeuralCIs

To train a NeuralCIs model, implement a subclass of `neuralcis.neuralcis.NeuralCIs`, construct an instance, and make a call to the method `fit()`.  Once fitting is completed, p-values and confidence intervals be easily generated by using the function `p_and_ci`.  To make a subclass of `NeuralCIs`, it is necessary to:

1. set values for the following instance variables:
   - `num_good_param`: At present this must be set to 1, and defines the number of quantities over which we are seeking simultaneous confidence intervals.
   - `num_known_param`: This defines how many other parameters there that are known a priori.  For example, sample group sizes would be an example of these.
2. implement the following member functions:
   - `simulate_sampling_distribution`: Generate samples from the sampling distribution.
   - `params_from_std_uniform`: Mapping from a set of standard uniform variables to an appropriate sampling of the parameters.
   - `params_from_std_uniform`: The inversion of the mapping in `params_from_std_uniform`.
   - `estimates_to_std_uniform`: A mapping from parameter ***estimates*** into a similar space to that used by `params_from_std_uniform`.  In most cases this can be just the same transform.

The process is better explained through a simple, trivial example:

### An Example

In the `examples/` folder, you will find the file `CohenDNeuralCIs.py` containing an object of the same name.  This class computes p-values and confidence intervals for a Cohen's *d*, calculated from the difference between two groups, relative to the pooled standard deviation.

You can see that `CohenDNeuralCIs` is a subclass of `neuralcis.neuralcis.NeuralCIs`, that `num_good_params` is set to 1 (as required), and that `num_known_params` is set to 2, since we have two known parameters: the number of participants in group 1, and the number of participants in group 2.  The "good" parameter that we are trying to estimate is the population value for Cohen's *d* (assuming that the SDs for the two groups are equal).  (These are called "good" parameters to distinguish them from nuisance parameters, that will be tackled in later versions).  There are also implementations of the above listed methods from `NeuralCIs`:

#### Methods Implemented from NeuralCIs

The method `simulate_sampling_distribution` takes four arguments: 
- self (which can be used to access quantities that you store within your class, but is not used here);
- pop_d, a Tensorflow `Tensor` of possible values for the population Cohen's *d*;
- n1, a Tensorflow `Tensor` of possible values for the sample size in group 1;
- n2, a Tensorflow `Tensor` of possible values for the sample size in group 2.

This is complete list of parameters to the model and, crucially, they have the "good" parameter (the one we are estimating first), and any known parameters second.  In all the functions in the class, we must use the parameters in the same order.  Each element of these tensors gives a particular `pop_d`, `n1` and `n2` that define a set of parameters from the model.  What the function does is from each element a *single* sample for the sample Cohen's *d* is drawn based on the parameters at that element in the `pop_d`, `n1` and `n2`.  So, what is returned is a Tensorflow `Tensor` of the same size as each of the inputs.

The actual sampling is done using helper functions provided in `neuralcis.sampling`, but it is also possible to write your own sampling function here.  However, it *must* be Tensorflow friendly, which means you will be limited to functions that you can find in the `tensorflow` and `tensorflow_probability` packages.  Nonetheless, these provide a huge set of statistical and sampling functions, so most possible use cases should be possible.

The remaining three functions are used to control how `NeuralCIs` samples the parameter values when training the network.  It is important that all possible parameter values that might be of interest are sampled from (with, ideally, a little margin around the edges).  It is also helpful to think about the sort of sampling that makes the most sense.  For example, sampling log standard deviations uniformly makes more sense than sampling standard deviations normally.

The `params_from_std_uniform` method controls the sampling process.  A series of Tensorflow `Tensor`s are passed into this function, one for each parameter, and they are populated with ***standard uniform*** random variables (selected uniformly from [0, 1]).  The job of this function then is to transform these uniform values into parameter values in the right range.  Again, the method in our example uses helper functions from `neuralcis.sampling` to transform the uniform variables into Cohen's *d*s between -2 and +2, and into group sample sizes between 3 and 300.

The next method, `params_to_std_uniform`, inverts the mapping above, and should be precisely an inverse of it.  The final method, `estimates_to_std_uniform` should in most cases be the same as `params_to_std_uniform`, but only applied to the "good" param.  It only needs to differ if not doing so would be expected to produce values far outside [0, 1].  

(The role of this last method is just to transform the estimates so that they are reasonably bounded on [0, 1], because the neural networks prefer numbers that are within similar bounds to each other; actually under the bonnet these will all be further transformed to be in [-1, 1], but that it is not necessary to know this).

#### Training and Running the Network

Now that it is implemented, it is just necessary to instantiate it and call the fit function:

    cohen_net = CohensDNeuralCIs()
    cohen_net.fit()

Once training is complete, it is possible to generate a *p*-value and confidence interval for a given sample by using the method `p_and_ci`, as follows:

    cohen_net.p_and_ci(0.3, 0.0, 10.0, 10.0, conf_level=.95)

Here, I asked for a *p*-value for a Cohen's *d* of 0.3, under a null hypothesis that the population *d* is 0.0, followed by the known parameters: there are 10 in each group.  Python replies with a *p*-value, and upper and lower confidence interval bounds, respectively (note that if you repeat this, you will get different numbers as there is a *lot* of randomness involved in training the net!):

```
(0.5172221660614014, -0.6254769563674927, 1.2101922035217285)
```

The `compute.es::des` function in `R` computes these same values, but using a very different method.  And the results are quite similar: 

```
compute.es::des(.3, 10, 10, level = 95)
Mean Differences ES:
    
d [ 95 %CI] = 0.3 [ -0.58 , 1.18 ] 
var(d) = 0.2 
p-value(d) = 0.51 
```

While this example shows just a more complicated way to do something that can already be done, the beauty of the neural method is that it can be applied to any quantity that you can write a sampling function for, which is not the case for the method used by the `R` function above.  Read on for more about the bigger picture...

## Motivation

The key driver for this project is my automated statistician, which will be hosted at my website, [statsadvice.com](https://statsadvice.com).  The automated statistician guides you through your own statistical output from your own experimental data, and explains what the numbers mean, along the way introducing more intuitive versions of the statistics ("common language effect sizes", and their confidence intervals).  

The problem I encountered was that there were various numbers I wanted the automated statistician to generate on the fly (particularly certain confidence intervals) that would need excessive amounts of computing power (killing my server, and leaving you waiting for your guided tour) and indeed, several for which I could not find any good method.  I realised that a neural network could be trained to calculate these values, and that once trained, the neural network could do so with relatively little computing power and in a fraction of a second.

## The Rough Idea

This is not simply a case of training a net on a set of inputs and outputs but, rather, the neural network works out the confidence intervals from first principles, based only on a simple simulation that is very easy to write.  Note that the following is currently a fairly terse, vague and technical description.  A better readme will follow as the project develops.

For example, suppose I want to generate Tukey confidence intervals for pairwise comparisons of a set of group means.  This is actually already rather easy without neural networks, but it is a great example that is widely understood.  I can provide the neural network with a simulation that randomly samples group means and standard deviations according to their sampling distribution, and a set of functions that describe how the pairwise differences are calculated from the group means.  The neural network does the rest, and returns a neural network that (within certain bounds on its inputs) can return, for any given set of sample group means and sds, simultaneous Tukey confidence intervals on the pairwise differences.

## Why Use Neural Networks?

The key intuition here is that a neural network is an extremely flexible *and differentiable* multivariate function approximator.  As such, this is not so much an application of neural networks as "artificial intelligence" as it use of a neural network as an extremely flexible regression.

The difficulty with calculating confidence intervals is that they effectively are calculated by searching all the possible "worlds" from which your sample could have been drawn, and finding *all* those which quite reasonably *could* have generated your data.  Statistics is full of all sorts of clever tricks for doing this, for example leveraging symmetries in how the world created your dataset to work backwards from your experimental results to the set of results that might have generated it.  

But for cases that fall outside those clever tricks, there are only a few general methods, which require a fair bit of number crunching, and are all built on approximations to some degree.  In the case where we want to generate confidence intervals for a set of numbers at the same time, the options are even thinner.

On the other hand, a neural network can be trained to form a compact representation of how all possible "real world"s will lead to different sorts of possible datasets, all in one, *easily differentiable* package.  Because it is so readily differentiated, another neural network can learn by searching through the different possible worlds that are encapsulated in this network.  

The current approach to neural CIs (in this project) works by training a whole series of networks in this way, to capture progressively more useful representations of how the world generates data, and finally working backwards by training a network that learns to invert this model to find what worlds could have generated any possible dataset.  By solving this for all possible datasets at once, a great deal of effort can be saved in the long run, since otherwise each confidence interval that is generated must repeat the same laborious work of searching through all the possible worlds that could have created it.

I will add a more detailed, and more intuitive explanation of this as the project develops.